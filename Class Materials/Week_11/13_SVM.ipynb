{"cells":[{"cell_type":"markdown","metadata":{"id":"pEUtyrDMBQ-H"},"source":["# Support Vector Machine\n","\n","## Table of Contents\n","\n","1. [**Introduction**](#Intro)   \n","2. [**Support Vector Machine**](#SVM)\n","3. [**Model Development**](#ModelDev)\n","  \n","    3.1. [**Steps to Implement**](#Proc)\n","  \n","    3.2. [**Hyper-parameters**](#Params) \n","\n","    3.3. [**Implementation**](#Impl)\n","\n","4. [**Model Evaluation**](#ModelEval)\n","5. [**One-Hot Encoding**](#OneHotEncode)\n","6. [**Final Comments**](#FinCom)"]},{"cell_type":"markdown","metadata":{"id":"VRaW1uFVF94c"},"source":["## 1 Introduction <a name=\"Intro\"></a>\n","\n","A classifier basically separates different classes in the data using __decision boundaries__ and by carving feature space into regions, so that all the points within any given region are destined to be assigned the same label. **Support Vector Machine (SVM)** can be viewed as a relative of logistic regression. __SVM__ is a supervised learning algorithm mostly used for classification.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SVa5bDI9OD4u"},"source":["\n","## 2 Support Vector Machine <a name=\"SVM\"></a>\n","In **SVM**, we are seeking to maximize the margin for the separator between the two classes. \n","\n","<img src=\"https://docs.google.com/uc?export=download&id=1Fg5h0FchJhWXvVsMpupTSqVgqn36aMpv\" width=\"700\">\n","\n","The channel between two classes is defined by a small number of data points as opposed to logistic regression, where all the points contribute to best position of the line. These contact points are the __support vectors__ defining the channels.\n","\n","\n","<img src=\"https://docs.google.com/uc?export=download&id=10EDJvJ7rGuMdLHNzPJjHt4VK8MfHJD6z\" width=\"500\">\n","\n","(ref: https://www.learnopencv.com/support-vector-machines-svm/)"]},{"cell_type":"markdown","metadata":{"id":"F8S9mB1cizSo"},"source":["## 3 Model Development <a name=\"ModelDev\"></a>\n","\n","## 3.1 Steps to Implement <a name=\"Proc\"></a>\n","Here are the steps to implement __SVM__ in Python using <font color='blue'>scikit-learn</font> library\n","\n","__1.__ Import `SVC`, `train_test_split`, and `MinMaxScaler` funcions from scikit learn library along with `numpy` library\n","```python\n","from sklearn.svm import SVC\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler  # For normalization\n","import numpy as np\n","```\n","\n","__2.__ Define dependant variable (target variable) and independent variables (features) from dataset:\n","```python\n","x_data=np.array(df[['feature1','feature2',...]])\n","y_data=df['target variable']\n","```\n","\n","__3.__ Normalize your data using `MinMaxScaler` (Optional but advised)\n","```python\n","MinMaxscaler = MinMaxScaler()  # define min max scaler\n","x_data_scaled = MinMaxscaler.fit_transform(x_data)  # transform data\n","```\n","\n","__4.__ Split the data into train and test sets: `x_train,x_test,y_train,y_test=train_test_split(x_data_scaled,y_data)`\n","\n","\n","\n","__5.__ Create an SVM classifier object using the constructor: `classifier = SVC(kernel = 'kernel type', C = numeric value, gamma = numeric value) `\n","\n","\n","__6.__ Use the fit function to fit the model to the training data: `classifier.fit(x_train,y_train)`\n","\n","__7.__ Then, make prediction using the test data and training data:\n","```python\n","yhatTest=classifier.predict(x_test)\n","yhatTrain=classifier.predict(x_train)\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"x11aP8D2ANA3"},"source":["### 3.2 Hyper-parameters <a name=\"Params\"></a>\n","\n","- <font color='red'> __C__ </font> (Regularization Parameter) tells the SVM optimization how much you want to avoid misclassifying each training example. If __C__ is high, the optimization will choose smaller margin hyperplane, so training data misclassification rate will be low. On the other hand, if __C__ is low, then the margin will be big, even if there will be misclassified training data examples.\n","\n","- <font color='red'> __Gamma__ </font>: The gamma parameter defines how far the influence of a single training example reaches. This means that high Gamma will consider only points close to the plausible hyperplane and low Gamma will consider points at greater distance. This parameter comes into play if only kernels 'rbf', 'poly' and 'sigmoid' are used."]},{"cell_type":"markdown","metadata":{"id":"hlM3hPXgCEKz"},"source":["### 3.3 Implementation <a name=\"Impl\"></a>\n","\n","\n","Steel plate faults dataset is provided by Semeion, Research of Sciences of Communication, Via Sersale 117, 00128, Rome, Italy. In this dataset, the faults of steel plates are classified into 7 types. Since it has been donated on October 26,2010, this dataset has been widely used in machine learning for automatic pattern recognition. Types of fault and corresponding numbers of sample are shown in the table below\n","\n","<img src=\"https://docs.google.com/uc?export=download&id=1pw1oJ7plDsTASg_ntI_QSVivQ-tMhlqq\" width=\"500\">\n","\n","\n","The number of samples vary a lot from one category to another. Meanwhile, fault 7 is a special class because it contains all other faults except the first six kinds of fault. In other words, samples in class 7 may have no obvious common characteristics. For every sample, 27 features are recorded, providing evidences for its fault class. All attributes are expressed by integers or real numbers. Detailed information about these 27 independent variables is listed out in the following table.\n","\n","<img src=\"https://docs.google.com/uc?export=download&id=1lAV-mPa2seL9VWkezbaCicnZVwOup2c6\" width=\"500\">\n","\n","\n","Ref: https://www.sciencedirect.com/science/article/pii/S0925231214012193?casa_token=8ZvcrfiUELkAAAAA:Vt2ShomuyzpagA6Su9nSQHzImgti_HHvtK5zuGqgC01It_Xn9UsccPB-5HVtzBonmsYCibDgYQ\n","\n","\n","\n","Ref for the dataset: https://archive.ics.uci.edu/ml/datasets/Steel+Plates+Faults\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j31hWnTpBJZE"},"outputs":[],"source":["import pandas as pd\n","\n","url = ('https://raw.githubusercontent.com/MasoudMiM/ME_364/main/Steel_Plates_Faults/Data.csv')\n","df = pd.read_csv(url,names=['X_Minimum', 'X_Maximum', 'Y_Minimum', 'Y_Maximum', 'Pixels_Areas', 'X_Perimeter',\n","                            'Y_Perimeter', 'Sum_of_Luminosity', 'Minimum_of_Luminosity', 'Maximum_of_Luminosity',\n","                            'Length_of_Conveyer', 'TypeOfSteel_A300', 'TypeOfSteel_A400', 'Steel_Plate_Thickness',\n","                            'Edges_Index', 'Empty_Index', 'Square_Index', 'Outside_X_Index', 'Edges_X_Index',\n","                            'Edges_Y_Index', 'Outside_Global_Index', 'LogOfAreas', 'Log_X_Index', 'Log_Y_Index',\n","                            'Orientation_Index', 'Luminosity_Index', 'SigmoidOfAreas', 'Pastry', 'Z_Scratch',\n","                            'K_Scratch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults'])           \n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gxrg2s4cSvMi"},"outputs":[],"source":["df.shape"]},{"cell_type":"markdown","metadata":{"id":"C1tEYATXsRg6"},"source":["__Step 1__, importing `SVC`, `train_test_split`, `MinMaxScaler`, and `numpy`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"97wDRFGxCks3"},"outputs":[],"source":["from sklearn.svm import SVC\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler  # For normalization\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"oG3PL8cxsnrw"},"source":["__Step 2__, defining target variable and independent variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kUvCsJmOCqIG"},"outputs":[],"source":["x_data=np.array(df[['X_Minimum', 'X_Maximum', 'Y_Minimum', 'Y_Maximum', 'Pixels_Areas', 'X_Perimeter',\n","             'Y_Perimeter', 'Sum_of_Luminosity', 'Minimum_of_Luminosity', 'Maximum_of_Luminosity',\n","             'TypeOfSteel_A300', 'TypeOfSteel_A400', 'Steel_Plate_Thickness']])\n","y_data=df['K_Scratch']"]},{"cell_type":"markdown","metadata":{"id":"En4Ia1w9s39I"},"source":["__Step 3__, normalizing using `MinMaxScaler`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l3qD__98Czj7"},"outputs":[],"source":["MinMaxscaler = MinMaxScaler()  # define min max scaler\n","x_data_scaled = MinMaxscaler.fit_transform(x_data)  # transform data"]},{"cell_type":"markdown","metadata":{"id":"P-1H0hXls_Q4"},"source":["__Step 4__, spliting the data into train and test sets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MsqZt4MIC1PU"},"outputs":[],"source":["x_train,x_test,y_train,y_test=train_test_split(x_data_scaled,y_data,test_size=0.3)"]},{"cell_type":"markdown","metadata":{"id":"-j7booHqtHl_"},"source":["__Step 5__, creating an SVM classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UrYIl25OC5Bw"},"outputs":[],"source":["classifier = SVC(kernel = 'rbf', C = 0.1,gamma=30)"]},{"cell_type":"markdown","metadata":{"id":"OPpyoPnOtPbx"},"source":["__Step 6__, fiting to the training data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e6rrdPzEC_61"},"outputs":[],"source":["classifier.fit(x_train,y_train)"]},{"cell_type":"markdown","metadata":{"id":"tVdvaWdOtYnk"},"source":["__Step 7__, making predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cBHo5ojzDBjM"},"outputs":[],"source":["yhatTest=classifier.predict(x_test)\n","yhatTrain=classifier.predict(x_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sTu9bxl-kZTg"},"outputs":[],"source":["classifier.classes_"]},{"cell_type":"markdown","metadata":{"id":"Sd4TClxpEPP0"},"source":["## 4 Model Evaluation <a name=\"ModelEval\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fD_70hiEEGvr"},"outputs":[],"source":["from sklearn.metrics import accuracy_score \n","from sklearn.metrics import jaccard_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import log_loss\n","\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import ConfusionMatrixDisplay"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9iQJJ3c1EdSU"},"outputs":[],"source":["acc_scoreTrain = accuracy_score(y_train,yhatTrain)\n","acc_scoreTest = accuracy_score(y_test,yhatTest)\n","print('accuracy for training data is %0.3f' %acc_scoreTrain)\n","print('accuracy for test data is %0.3f' %acc_scoreTest)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"54ENpsEFFAoX"},"outputs":[],"source":["J_scoreTrain = jaccard_score(y_train,yhatTrain)\n","J_scoreTest = jaccard_score(y_test,yhatTest)\n","print('Jaccard Index for training data is %0.3f' %J_scoreTrain)\n","print('Jaccard Index for test data is %0.3f' %J_scoreTest)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LeIeZ6dCFFn2"},"outputs":[],"source":["F_scoreTrain = f1_score(y_train,yhatTrain)\n","F_scoreTest = f1_score(y_test,yhatTest)\n","print('F-Score for training data is %0.3f' %F_scoreTrain)\n","print('F-Score for test data is %0.3f' %F_scoreTest)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_BRLLrS1BNWK"},"outputs":[],"source":["LogLossTrain = log_loss(y_train,yhatTrain)\n","LogLossTest = log_loss(y_test,yhatTest)\n","print('F-Score for training data is %0.3f' %LogLossTrain)\n","print('F-Score for test data is %0.3f' %LogLossTest)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"snK5-QSIAzKh"},"outputs":[],"source":["print('Confusion matrix for training data')\n","CM_scoreTrain = confusion_matrix(y_train,yhatTrain)   # possible option normalize='true'\n","print(CM_scoreTrain)\n","\n","print(40*'-')\n","\n","print('Confusion matrix for test data')\n","CM_scoreTest = confusion_matrix(y_test,yhatTest)   # possible option normalize='true'\n","print(CM_scoreTest)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SEC5h_wyFHvQ"},"outputs":[],"source":["dispTr = ConfusionMatrixDisplay(CM_scoreTrain,display_labels=['No Fault','Fault'])\n","dispTr.plot()\n","\n","dispTs = ConfusionMatrixDisplay(CM_scoreTest,display_labels=['No Fault','Fault'])\n","dispTs.plot()"]},{"cell_type":"markdown","metadata":{"id":"zYzkxcxECtSr"},"source":["<font color='red'>__IMPORTANT NOTE:__</font> SVM algorithm can also be used for multiclass classification, similar to the approach followed when implmeneting logistic regression algorithm for multiclass classification."]},{"cell_type":"markdown","metadata":{"id":"VSvYiOv_Is95"},"source":["## 5 One-Hot Encoding <a name=\"OneHotEncode\"></a>\n","\n","Many machine learning algorithms cannot operate on labeled data directly. They require all input variables and output variables to be numeric. In some cases when we have categorical data and we want to use binary classification, we need to convert that categorical variable into integer values.\n","\n","One way to do this is to use __one-hot encoding__. For instance for a categorical variable with three possible classes, we need three new variables with possible values of 0 and 1.\n","\n","Let's demonstrate that using an example with part of the Fuel Economy Data Set, which is produced by the Office of Energy Efficiency and Renewable Energy of the U.S. Department of Energy. Fuel economy data are the result of vehicle testing done at the Environmental Protection Agency's National Vehicle and Fuel Emissions Laboratory in Ann Arbor, Michigan, and by vehicle manufacturers with oversight by EPA. This data set can be accessed from here: https://github.com/MasoudMiM/ME_364/blob/main/EPA_Green_Vehicle_Guide/EPA_2020_Fuel_Economy.csv and a description of the data is provided at this link: https://www.fueleconomy.gov/feg/EPAGreenGuide/GreenVehicleGuideDocumentation.pdf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VtFJSVy8ILSR"},"outputs":[],"source":["import pandas as pd\n","\n","url = ('https://raw.githubusercontent.com/MasoudMiM/ME_364/main/EPA_Green_Vehicle_Guide/EPA_2020_Fuel_Economy.csv')\n","dfFuel = pd.read_csv(url)           \n","\n","dfFuel.drop(columns='Unnamed: 0',inplace=True)\n","dfFuel.head()"]},{"cell_type":"markdown","metadata":{"id":"W0QiuBsSL0SY"},"source":["The column __SmartWay__ is a categorical data and has three possible classes, __No__, __Yes__, and __Elite__. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BUJr1wTvL_ck"},"outputs":[],"source":["dfFuel['SmartWay'].unique()"]},{"cell_type":"markdown","metadata":{"id":"N-FUv5biMPi6"},"source":["We can transform this variable into a vector of three numerical values for each row with 1 if that row belongs to a specific class and 0 if it does not. This can be done using <font color='blue'> Pandas</font>'s library function `get_dummies`.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lSyZ-xJUNZNr"},"outputs":[],"source":["dfFuel_encoded = pd.get_dummies(data=dfFuel,columns=['SmartWay'])\n","dfFuel_encoded.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q5iwvxyqV57y"},"outputs":[],"source":["dfFuel_encoded.tail()"]},{"cell_type":"markdown","metadata":{"id":"zfvBmv08RYYW"},"source":["Or you can do it for __Fuel__ column with two unique classes of __Gasoline__ and __Diesel__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UNr9ed3mRmJ5"},"outputs":[],"source":["dfFuel['Fuel'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T65NImFWSPQf"},"outputs":[],"source":["dfFuel_encoded2 = pd.get_dummies(data=dfFuel,columns=['Fuel'])\n","dfFuel_encoded2.head()"]},{"cell_type":"markdown","metadata":{"id":"2r7MGGCjOkzo"},"source":["This increses the number of columns (dimensions of your dataset) and as a result the size of your dataframe but your learning algorithm will perform a lot better."]},{"cell_type":"markdown","metadata":{"id":"CbeMCJwRGkHb"},"source":["## 6 Final Comments <a name=\"FinCom\"></a>\n","\n","- SVM can be very efficient, because it uses only a subset of the training data, only the support vectors\n","- It can have high accuracy, sometimes can perform even better than neural networks\n","- Not very sensitive to overfitting\n","- Training time is high when we have large datasets\n","- When the dataset has more noise (i.e. target classes are overlapping) SVM doesnâ€™t perform well\n","- One hot encoding is used to convert categorical data into integer data to be used for classification purposes"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"12_SVM.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat":4,"nbformat_minor":0}
